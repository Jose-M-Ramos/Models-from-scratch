import numpy as np

# Load the MNIST dataset
(x_train, y_train), (x_test, y_test) = np.load('mnist.npz', allow_pickle=True)['arr_0']

# Normalize the pixel values
x_train = x_train / 255.0
x_test = x_test / 255.0

# Flatten the images into 1D arrays
x_train = x_train.reshape(-1, 28*28)
x_test = x_test.reshape(-1, 28*28)

# One-hot encode the labels
y_train = np.eye(10)[y_train]
y_test = np.eye(10)[y_test]

# Set the learning rate
learning_rate = 0.01

# Set the number of epochs
num_epochs = 5

# Set the batch size
batch_size = 64

# Set the number of hidden units
num_units = 128

# Initialize the weights and biases
w1 = np.random.randn(28*28, num_units)
b1 = np.zeros(num_units)
w2 = np.random.randn(num_units, 10)
b2 = np.zeros(10)

# Define the sigmoid activation function
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Define the softmax activation function
def softmax(x):
    exp = np.exp(x - np.max(x))
    return exp / np.sum(exp, axis=1, keepdims=True)

# Define the cross-entropy loss function
def cross_entropy_loss(y_pred, y_true):
    return -np.sum(y_true * np.log(y_pred), axis=1)

# Define the mean squared error loss function
def mean_squared_error_loss(y_pred, y_true):
    return 0.5 * np.sum((y_pred - y_true)**2, axis=1)

# Define the forward pass
def forward_pass(x, w1, b1, w2, b2):
    z1 = x.dot(w1) + b1
    a1 = sigmoid(z1)
    z2 = a1.dot(w2) + b2
    a2 = softmax(z2)
    return z1, a1, z2, a2

# Define the backward pass
def backward_pass(x, y, z1, a1, z2, a2, w1, b1, w2, b2):
    m = x.shape[0]
    dz2 = a2 - y
    dw2 = a1.T.dot(dz2) / m
    db2 = np.sum(dz2, axis=0) / m
    da1 = dz2.dot(w2.T)
    dz1 = da1 * sigmoid(z1) * (1 - sigmoid(

